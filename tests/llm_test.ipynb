{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock LLM\n",
    "bedrock_llm = Bedrock(\n",
    "    client=boto3.client('bedrock-runtime'), model_id=\"anthropic.claude-v2:1\"  # Replace with your AWS Region\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/q1p_lrjj0ws6gc5nvn1dy_mh0000gn/T/ipykernel_74269/4158542865.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/q1p_lrjj0ws6gc5nvn1dy_mh0000gn/T/ipykernel_74269/2871230382.py:5: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = bedrock_llm(query)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Simple input for testing dialogue\n",
    "# query = \"How are you?\"\n",
    "\n",
    "# # Run the query through the Bedrock model\n",
    "# response = bedrock_llm(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the template for generating descriptions\n",
    "template = \"\"\"\n",
    "    Given a word {korean_word} and its meaning in Japanese is {japanese_translation}, \n",
    "    please create a description of the word. Identify the following:\n",
    "\n",
    "    1. Part of Speech: Identify the word's role (Noun, Verb, Adjective, etc.).\n",
    "    2. Usage Context: Describe in which situations or contexts it can be used.\n",
    "    3. Synonyms and Related Terms: Mention any synonyms in Korean or Japanese if applicable.\n",
    "    4. Example Sentence: Provide a simple sentence in Korean using the word.\n",
    "\n",
    "    ### Output:\n",
    "    - Description: {description}\n",
    "    - Part of Speech: {part_of_speech}\n",
    "    - Usage Context: {usage_context}\n",
    "    - Synonyms: {synonyms}\n",
    "    - Example Sentence: {example_sentence}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「歌手」\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simplified template for dialogue generation\n",
    "template = \"\"\"\n",
    "とある単語は入力されます。\n",
    "韓国語であは「{korean_word}」そしてその日本語では「{japanese_translation}」。\n",
    "あなたは辞書です。この単語の意味を示してください、ひと文で。\n",
    "品詞などの情報はいらない。\n",
    "できるだけ短いに。\n",
    "\n",
    "### 例：\n",
    "가다を入力すると、「移動する、進む」をアウトプットする\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the LangChain prompt template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create an LLM Chain with the template and Bedrock LLM\n",
    "description_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "# Function to generate description\n",
    "def generate_korean_word_description(korean_word, japanese_translation):\n",
    "    # Use the LLM chain to generate the description based on the inputs\n",
    "    response = description_chain.run(korean_word=korean_word, japanese_translation=japanese_translation)\n",
    "    return response\n",
    "\n",
    "# Test the generator with a simplified input\n",
    "korean_word = \"가수\"\n",
    "japanese_translation = \"歌手\"\n",
    "output = generate_korean_word_description(korean_word, japanese_translation)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
